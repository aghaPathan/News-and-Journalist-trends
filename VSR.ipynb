{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import porter\n",
    "from nltk.stem import PorterStemmer\n",
    "from numpy import zeros,dot\n",
    "from numpy.linalg import norm\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<WordListCorpusReader in '.../corpora/stopwords' (not loaded yet)>\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = re.compile ( \"[a-z\\-']+\", re.I )\n",
    "stemmer= PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "all_words=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word(word,d):\n",
    "    w=word.lower() \n",
    "    if w not in stop_words:\n",
    "        ws=stemmer.stem(w,0,len(w)-1)\n",
    "        d.setdefault(ws,0)\n",
    "        d[ws] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_vec(doc,key_idx):\n",
    "    v=zeros(len(key_idx))\n",
    "    for word in splitter.findall(doc):\n",
    "        keydata=key_idx.get(stemmer.stem(word,0,len(word)-1).lower(), None)\n",
    "        if keydata: v[keydata[0]] = 1\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare(doc1,doc2):\n",
    "\n",
    " # strip all punctuation but - and '\n",
    " # convert to lower case\n",
    " # store word/occurance in dict\n",
    "     all_words=dict()\n",
    "\n",
    "     for dat in [doc1,doc2]:\n",
    "        [add_word(w,all_words) for w in splitter.findall(dat)]\n",
    "\n",
    "     # build an index of keys so that we know the word positions for the vector\n",
    "     key_idx=dict() # key-> ( position, count )\n",
    "     keys=all_words.keys()\n",
    "     keys.sort()\n",
    "     #print keys\n",
    "     for i in range(len(keys)):\n",
    "      key_idx[keys[i]] = (i,all_words[keys[i]])\n",
    "     del keys\n",
    "     del all_words\n",
    "\n",
    "     v1=doc_vec(doc1,key_idx)\n",
    "     v2=doc_vec(doc2,key_idx)\n",
    "     return float(dot(v1,v2) / (norm(v1) * norm(v2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
